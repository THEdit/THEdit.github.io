<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>THEdit: Talking-Head Video Editing via Context-aware Animation Prediction and NeRF-based Rendering</title>
    <link href="./assets/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="./assets/font.css">
</head>

<section>
    <div class="jumbotron text-center mt-4">
        <div class="container">
            <div class="row">
                <div class="col-12">
                    <h1><b>THEdit: Talking-Head Video Editing via Context-aware Animation Prediction and NeRF-based Rendering</b>
                    </h1>
                    <h5>
                        <a>Anonymous CVPR submission</a>
                        &nbsp;&nbsp;&nbsp;&nbsp;

<!--                         <a href="https://tan-xu.github.io/" target="_blank">Xu Tan</a><sup>2</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp;

                        <a href="https://scholar.google.com/citations?user=jk6jWXgAAAAJ" target="_blank">Liyang Chen</a><sup>3</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp;

                        <a href="https://scholar.google.com/citations?user=UlMO-z8AAAAJ" target="_blank">Runnan Li</a><sup>4</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp;

                        <a href="" target="_blank">Yuchao Zhang</a><sup>4</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp;

                        <a href="https://scholar.google.com/citations?user=689bIIwAAAAJ" target="_blank">Sheng Zhao</a><sup>4</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp;
                        
                        <a href="https://medialab.sjtu.edu.cn/" target="_blank">Li Song</a><sup>1</sup>
                        &nbsp;&nbsp;&nbsp;&nbsp; -->
                    </p>
                        <a>Paper ID 1688</a>&nbsp;&nbsp;&nbsp;
<!--                         <a><sup>2</sup>Microsoft Research Asia</a>&nbsp;&nbsp;&nbsp;
                        <a><sup>3</sup>Tsinghua University</a>&nbsp;&nbsp;&nbsp;
                        <a><sup>4</sup>Microsoft Azure Speech</a> -->
                    </h5>
                </div>
            </div>
        </div>
    </div>
</section>


<section>
    <div class="container">
        <h3><b>Pipeline</b></h3>
        <!-- <center> -->
        <div class="row">
            <div class="col-12 text-center">
                <img src="media/framework.svg" width="800"></img>
                <!-- <p><b>Fig1. Framework Overview</b></p> -->
            </div>
        </div>
        <!-- </center> -->
        <!-- <div class="row">
            <div class="col-12 text-center">
                <img src="media/details.svg" width="900"></img>
                <p><b>Fig2. Details of each component.</b></p>
            </div>
        </div> -->
    </div>
</section>


<section>
    <div class="container"><br></br>
        <h3><b>Abstract</b></h3>
        <div class="row">
            <div class="col-12 text-center">
                <p class="text-justify">
                    Talking-head video editing aims to insert, delete, and substitute the word of a pre-recorded video through text transcript editor in few-shot setting. The key challenge for this task is to obtain an editing model that generates new talking-head video clips which has not only accurate lip synchronization but also motion smoothness. Previous approaches, including 3DMM-based editing methods and NeRF-based (Neural Radiance Field) methods, have been attracting increasing attention in recent years. However, these methods are sub-optimal in that they either require minutes of source videos and days of training time, or lack the fine-grained control of verbal (e.g., lip motion) and non-verbal (e.g., head pose and expression) representations. In this work, we propose a context-aware editing framework for word-level talking-head video editing. First, we design a sequential animation prediction module which fully utilizes the context information and the prior knowledge to improve lip synchronization towards the novel speech audio. Then, we introduce a NeRF-based rendering module to synthesize high-fidelity edited video given the predicted animations. Our rendering module capture visual features from the adjacent frames as reference to maintain the identity and lighting consistency. Extensive experiments demonstrate that our method achieves smoother editing results with higher image quality and lip accuracy in less training data than previous methods.
            </div>
        </div>
    </div>
</section>


<section>
    <div class="container"><br></br>
        <h3><b>Demo Videos</b></h3>
        <div class="row">
            <div class="col-12 text-center">
                <video width="800" height="500" src="media/Supp_demo.mp4" allowfullscreen controls></video>
            </div>
        </div>
    </div>
</section>


<section>
    <div class="container"><br></br>
        <h3><b>Videos of Figure 1</b></h3>
        <div class="row">
            <div class="col-12 text-center">
                <video width="800" height="500" src="media/Supp_Fig1.mp4" allowfullscreen controls></video>
                <p></p>
            </div>
        </div>
    </div>
</section>



<section>
    <div class="container"><br></br>
        <h3><b>Materials</b></h3>
        <center>
        <div class="row">
            <ul class="col-12 text-center">
                <li>
                    <a href="https://arxiv.org/abs/2208.13717" target="_blank">
                        <image src="media/paper_image.jpg" height="90px">
                        <p>arXiv</p>
                    </a>
                </li>
            </ul>
        </div>
    </center>
    </div>
</section>


<!-- citing -->
<div class="container"><br></br>
    <div class="row ">
        <div class="col-12">
        <h3><b>Citation</b></h3>
        <pre style="background-color: #edf0f0;padding: 1.25em 1.5em"><code>@article{ling2022stableface,
  title={StableFace: Analyzing and Improving Motion Stability for Talking Face Generation},
  author={Ling, Jun and Tan, Xu and Chen, Liyang and Li, Runnan and Zhang, Yuchao and Zhao, Sheng and Song, Li},
  journal={arXiv preprint arXiv:2208.13717},
  year={2022}
}</code>
</pre>
<hr>
</div>
</div>
</div>

</html>
